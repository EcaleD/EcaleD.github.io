[{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nAutomate testing in the technical documentation workflow Keeping technical documentation up‑to‑date is a challenging task. It requires a mechanism to detect discrepancies between the product and the documentation. Without a procedural guarantee, existing docs can be overlooked, since documenting new features takes priority.\nOne advantage of doc-as-code is that it brings documentation closer to the product code, thereby making it more likely to be built and tested automatically.\nBackground I worked as a technical writer at a database company last year. Occasionally, code updates would break key procedures described in the documentation.\nAs we just shifted the docs to Gitlab and started adopting a doc-as-code workflow, I experimented with an automation testing pipeline inspired by our development team’s CI (Continuous Integration), which automatically runs database regression tests on a VM whenever an update was pushed to the main branch.\nThis is not a step-by-step tutorial but more of a project summary, as the process contains steps that only apply to a certain type of database docs. But I think the idea can be generalized and potentially applied to doc workflows facing similar maintenance issues.\nWhat is CI? If you’re unfamiliar with Continuous Integration (CI), it\u0026rsquo;s a process that allows developers to merge changes into the main codebase frequently. CI includes automated tests that verify the changes and ensure no conflicts arise.\nIn addition, my project leverages the PostgreSQL regression testing tool, pg_regress, to do the actual testing work. With pg_regress you simply create test cases in SQL; the tool runs them and returns the results and debugging information.\nFlow Here’s how the automation test works:\n1 2 3 DB updated → CI pipeline triggered → (In VM) Pull test cases → Install the latest DB → Run the tests → Return test results → Clean up the env Below is a breakdown of how it was established:\nPrerequisites Setting up a full test framework or pipeline from scratch isn’t realistic. So this trial relies on a few prerequisites:\nYour development team already has a CI pipeline. Test cases are not difficult to be created. In my case, the test tool uses SQL syntax, which isn’t too complex for a database tech writer. You have access and permissions to required resources (e.g., VM creation in the dev/test environment). Step 1: Set up a VM for testing First, you need an environment to run tests. For my case, the pipeline uses pg_regress to execute SQL scripts on a live database. So I needed a VM where the pipeline could install the latest DB, add test cases, and run the tests.\nWith the help of a test engineer, I got a script that set up and initializes the regression test flow. (This can be done by a writer too, through reading the code of the test pipeline. But it would take much longer.)\nYou’ll need to check with your dev team about how they manage test environments and mirror the configuration and the launch method.\nStep 2: Prepare the test script Next, you’ll need to create a script to do the actual job. The Gitlab CI will pick up and execute the script when the condition you predefined is met.\nMy script does the following:\nDownload test cases from the documentation GitLab repo and move them to the specified folder in the VM.\nExample code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 INPUT_DIR=\u0026#34;[source_sql_directory]\u0026#34; EXPECTED_DIR=\u0026#34;[source_expected_output_directory]\u0026#34; SQL_DIR=\u0026#34;[destination_sql_directory]\u0026#34; EXPECTED_DEST_DIR=\u0026#34;[destination_expected_output_directory]\u0026#34; DEBUG_SCHEDULE=\u0026#34;[test_schedule_file]\u0026#34; # Move source .sql files and update debug_schedule move_and_update_sql() { local src_dir=$1 local dest_dir=$2 for file in \u0026#34;$src_dir\u0026#34;/*; do [ -f \u0026#34;$file\u0026#34; ] || continue # Skip if not a file file_name=$(basename \u0026#34;$file\u0026#34;) file_base_name=\u0026#34;${file_name%.*}\u0026#34; # Strip the file extension if [ ! -f \u0026#34;$dest_dir/$file_name\u0026#34; ]; then mv \u0026#34;$file\u0026#34; \u0026#34;$dest_dir/\u0026#34; echo \u0026#34;test: $file_base_name\u0026#34; \u0026gt;\u0026gt; \u0026#34;$DEBUG_SCHEDULE\u0026#34; echo \u0026#34;Moved $file_name to $dest_dir and updated debug_schedule.\u0026#34; fi done } # Move source expected output files move_and_update_expected() { local src_dir=$1 local dest_dir=$2 for file in \u0026#34;$src_dir\u0026#34;/*; do [ -f \u0026#34;$file\u0026#34; ] || continue # Skip if not a file file_name=$(basename \u0026#34;$file\u0026#34;) if [ ! -f \u0026#34;$dest_dir/$file_name\u0026#34; ]; then mv \u0026#34;$file\u0026#34; \u0026#34;$dest_dir/\u0026#34; echo \u0026#34;Moved $file_name to $dest_dir.\u0026#34; fi done } move_and_update_sql \u0026#34;$INPUT_DIR\u0026#34; \u0026#34;$SQL_DIR\u0026#34; move_and_update_expected \u0026#34;$EXPECTED_DIR\u0026#34; \u0026#34;$EXPECTED_DEST_DIR\u0026#34; Download and install the latest database version.\n1 2 3 4 url=\u0026#34;[DB_install_package]\u0026#34; filename=$(basename \u0026#34;$url\u0026#34;) wget \u0026#34;$url\u0026#34; rpm -ivh \u0026#34;$filename\u0026#34; Run the tests and print the results.\nClean up the environment.\nStep 3: Register a GitLab Runner GitLab uses runners to execute CI/CD tasks in a given env defined in the .gitlab-ci.yml file in each repo. You define how the runner is triggered and what it would do in your test env.\nInstall GitLab Runner on the VM:\n1 2 3 4 5 6 # Add the GitLab Runner repository curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash # Install GitLab Runner sudo apt-get update sudo apt-get install -y gitlab-runner Register the Runner:\nGet a registration token from your GitLab project: Settings \u0026gt; CI/CD \u0026gt; Runners \u0026gt; Expand \u0026ldquo;Set up a specific Runner manually\u0026rdquo;. On the VM, run: sudo gitlab-runner register, then follow the prompts to complete the registration: GitLab instance URL: e.g., https://gitlab.com or your self-hosted instance Registration token: from the UI Description: a name to identify the runner Tags: optional Executor: enter shell Step 4: Create a .gitlab-ci.yml File This file defines your CI pipeline. Place it in the root of your documentation repo. You need to put the script you created in Step 2 to the script section in the yml file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 stages: - test centos7:x86_64:icw: stage: test script: | - echo \u0026#34;Running tests\u0026#34; # Add the script here rules: - changes: - test_case/**/* timeout: 8 hours allow_failure: false variables: CI_DEBUG_TRACE: \u0026#34;true\u0026#34; By now, the pipeline has been built up. The next thing is to create test cases and get the pipeline run when DB is updated!\nWrite the test cases pg_regress makes it simple to write tests. For each test case, it requires two files:\nA .sql file that contains the SQL commands to be run A .out file that contains the expected output To verify the test cases:\nCheck regression.diffs to compare expected vs actual output (watch out for extra spaces at line edges). Inspect test output in the results folder. After a test case is created, you can just add them to the doc repo, the pipeline will automatically pick them up when triggered.\nWrap-up By this point, the initial version of the test pipeline is set up. The last thing is to set a proper trigger for the pipeline, for example, when the new DB version is out, the pipeline is triggered and also pulls the URL to the new DB’s installation package.\nThis pipeline is far from perfect and could be improved in all aspects. It’s just a proof-of-concept of a development-styled test process for docs. When manual maintenance becomes unmanageable, it\u0026rsquo;s worth trying to set up such an automation to help.\nImproving points:\nAI could be leveraged to convert existing tutorials into test cases, which would significantly expand the scope of the test. Enterprise databases often integrate with broader architectures (e.g., Hadoop or Hive). If the workflow can set up environments for those integration use cases, more complex tutorials could be included in the testing. ","date":"2025-08-27T00:00:00Z","image":"http://localhost:1313/p/automate-testing-in-the-technical-documentation-workflow/header_image_by_gpt_hu_bb6755a64982bc13.png","permalink":"http://localhost:1313/p/automate-testing-in-the-technical-documentation-workflow/","title":"Automate testing in the technical documentation workflow"}]